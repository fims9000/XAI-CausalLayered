#!/usr/bin/env python3
"""
Analysis Module for Heart Disease Diagnosis System
================================================

Contains machine learning analysis components including:
- XGBoost Model with temperature scaling
- SHAP Analysis
- ANFIS Model
- Rule Aggregation and Consensus
- Uncertainty Quantification
"""

import numpy as np
import pandas as pd
import xgboost as xgb
import shap
from typing import Dict, List, Tuple, Optional
from sklearn.metrics import accuracy_score
from sklearn.ensemble import IsolationForest
from utils import Colors, SafeMedicalReporting, check_library_availability
from config import UncertaintyMetrics
from models import mcnemar_test_simple

# Check library availability
AVAILABILITY = check_library_availability()
XANFIS_AVAILABLE = AVAILABILITY.get('XANFIS', False)
SCIPY_AVAILABLE = AVAILABILITY.get('SCIPY', False)
ADVANCED_ML_AVAILABLE = AVAILABILITY.get('ADVANCED_ML', False)

if XANFIS_AVAILABLE:
    from xanfis import GdAnfisClassifier

class XGBoostModel:
    """High-performance XGBoost classifier with modern calibration methods.
    
    References:
    - Guo et al., On Calibration of Modern Neural Networks, ICML 2017
    - Kull et al., Beta calibration, AISTATS 2017
    """
    
    def __init__(self):
        self.model = None
        self.calibrated_model = None  # Temperature scaled model
        self.temperature = 1.0  # Temperature parameter
        self.feature_importances = None
        self.test_accuracy = 0.0
        self.probabilities = None
        self.calibrated_probabilities = None
        
    def train(self, X_train, y_train, X_test, y_test) -> bool:
        """Train XGBoost model."""
        try:
            print(f"  {Colors.PROGRESS}üöÄ Training XGBoost...{Colors.RESET}")
            
            self.model = xgb.XGBClassifier(
                n_estimators=200, max_depth=6, learning_rate=0.1,
                subsample=0.8, colsample_bytree=0.8, random_state=42
            )
            
            self.model.fit(X_train, y_train)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º y_test –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
            self.y_test_actual = y_test
            
            # Evaluate
            predictions = self.model.predict(X_test)
            self.probabilities = self.model.predict_proba(X_test)[:, 1]
            self.test_accuracy = accuracy_score(y_test, predictions)
            
            # Feature importance
            self.feature_importances = pd.DataFrame({
                'feature': X_train.columns,
                'importance': self.model.feature_importances_
            }).sort_values('importance', ascending=False)
            
            # Apply temperature scaling calibration (Guo et al., 2017)
            self.temperature_scaling_calibration(X_test, y_test)

            print(f"  {Colors.SUCCESS}‚úÖ XGBoost trained - Accuracy: {self.test_accuracy:.3f}, Temperature: {self.temperature:.3f}{Colors.RESET}")
            return True
            
        except Exception as e:
            print(f"  {Colors.ERROR}‚ùå XGBoost training failed: {str(e)}{Colors.RESET}")
            return False
    
    def display_results(self):
        """Display XGBoost results."""
        print(f"\n{Colors.SUCCESS}üéØ XGBOOST RESULTS{Colors.RESET}")
        print(f"Test Accuracy: {self.test_accuracy:.4f} ({self.test_accuracy*100:.2f}%)")
        
        print(f"\n{Colors.INFO}üìä TOP 5 FEATURE IMPORTANCES{Colors.RESET}")
        for i, (_, row) in enumerate(self.feature_importances.head(5).iterrows(), 1):
            print(f"{i}. {row['feature']:<25} {row['importance']:.4f}")
        
        # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self.plot_model_performance()
    
    def plot_model_performance(self):
        """–ì—Ä–∞—Ñ–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ XGBoost –º–æ–¥–µ–ª–∏."""
        try:
            import matplotlib.pyplot as plt
            import seaborn as sns
            from sklearn.metrics import roc_curve, auc, calibration_curve
            
            print(f"  {Colors.INFO}üìä Creating performance plots...{Colors.RESET}")
            
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle('XGBoost Model Performance Analysis', fontsize=16, fontweight='bold')
            
            # 1. ROC Curve
            fpr, tpr, _ = roc_curve(self.y_test_actual, self.probabilities)
            roc_auc = auc(fpr, tpr)
            
            axes[0,0].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC Curve (AUC = {roc_auc:.3f})')
            axes[0,0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
            axes[0,0].set_xlabel('False Positive Rate')
            axes[0,0].set_ylabel('True Positive Rate')
            axes[0,0].set_title('ROC Curve')
            axes[0,0].legend()
            axes[0,0].grid(True)
            
            # 2. Feature Importance
            top_features = self.feature_importances.head(8)
            axes[0,1].barh(top_features['feature'], top_features['importance'], color='skyblue')
            axes[0,1].set_xlabel('Feature Importance')
            axes[0,1].set_title('Top-8 Important Features')
            axes[0,1].grid(True, axis='x')
            
            # 3. Probability Distribution
            axes[1,0].hist(self.probabilities[self.y_test_actual == 0], bins=20, alpha=0.7, 
                           label='Healthy', color='green', density=True)
            axes[1,0].hist(self.probabilities[self.y_test_actual == 1], bins=20, alpha=0.7, 
                           label='Disease', color='red', density=True)
            axes[1,0].set_xlabel('Predicted Probability')
            axes[1,0].set_ylabel('Density')
            axes[1,0].set_title('Prediction Distribution')
            axes[1,0].legend()
            axes[1,0].grid(True)
            
            # 4. Calibration Plot
            prob_true, prob_pred = calibration_curve(self.y_test_actual, self.probabilities, n_bins=10)
            axes[1,1].plot(prob_pred, prob_true, marker='o', linewidth=2, label='Calibration')
            axes[1,1].plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfect Calibration')
            axes[1,1].set_xlabel('Mean Predicted Probability')
            axes[1,1].set_ylabel('Fraction of Positives')
            axes[1,1].set_title('Calibration Plot')
            axes[1,1].legend()
            axes[1,1].grid(True)
            
            plt.tight_layout()
            plt.show()
            print(f"  {Colors.SUCCESS}‚úÖ Performance plots created{Colors.RESET}")
            
        except Exception as e:
            print(f"  {Colors.WARNING}‚ö†Ô∏è Failed to create performance plots: {str(e)}{Colors.RESET}")
    
    def temperature_scaling_calibration(self, X_val: np.ndarray, y_val: np.ndarray) -> float:
        """Apply temperature scaling calibration following Guo et al., 2017."""
        try:
            print(f"  {Colors.PROGRESS}üå°Ô∏è Applying temperature scaling calibration...{Colors.RESET}")
            
            if not SCIPY_AVAILABLE:
                print(f"  {Colors.WARNING}‚ö†Ô∏è SciPy not available, using default temperature=1.0{Colors.RESET}")
                self.temperature = 1.0
                self.calibrated_probabilities = self.probabilities
                return 1.0
            
            # Get uncalibrated probabilities (logits)
            if self.model is None:
                print(f"  {Colors.WARNING}‚ö†Ô∏è Model not trained, skipping calibration{Colors.RESET}")
                return 1.0
            
            # Get raw predictions (convert probabilities back to logits)
            raw_probs = self.model.predict_proba(X_val)[:, 1]
            
            # Convert probabilities to logits for temperature scaling
            eps = 1e-10
            raw_probs = np.clip(raw_probs, eps, 1-eps)
            logits = np.log(raw_probs / (1 - raw_probs))
            
            # Define negative log-likelihood objective
            def negative_log_likelihood(temperature):
                if temperature <= 0:
                    return 1e10  # Invalid temperature
                
                scaled_logits = logits / temperature
                scaled_probs = 1 / (1 + np.exp(-scaled_logits))  # Sigmoid
                scaled_probs = np.clip(scaled_probs, eps, 1-eps)
                
                # Binary cross-entropy loss
                nll = -np.mean(y_val * np.log(scaled_probs) + (1 - y_val) * np.log(1 - scaled_probs))
                return nll
            
            # Optimize temperature using scipy.optimize
            from scipy.optimize import minimize_scalar
            
            result = minimize_scalar(
                negative_log_likelihood,
                bounds=(0.1, 10.0),
                method='bounded',
                options={'xatol': 1e-6}
            )
            
            self.temperature = result.x if result.success else 1.0
            
            # Apply calibration to get calibrated probabilities
            scaled_logits = logits / self.temperature
            self.calibrated_probabilities = 1 / (1 + np.exp(-scaled_logits))
            
            # Calculate Expected Calibration Error (ECE)
            ece_before = self._calculate_ece(raw_probs, y_val)
            ece_after = self._calculate_ece(self.calibrated_probabilities, y_val)
            
            print(f"  {Colors.INFO}üå°Ô∏è Optimal Temperature: {self.temperature:.4f}{Colors.RESET}")
            print(f"  {Colors.INFO}üé° ECE Before: {ece_before:.4f}, After: {ece_after:.4f}{Colors.RESET}")
            
            return self.temperature
            
        except Exception as e:
            print(f"  {Colors.ERROR}‚ùå Temperature scaling failed: {str(e)}{Colors.RESET}")
            self.temperature = 1.0
            self.calibrated_probabilities = self.probabilities
            return 1.0
    
    def _calculate_ece(self, probabilities: np.ndarray, y_true: np.ndarray, n_bins: int = 10) -> float:
        """Calculate Expected Calibration Error (ECE)."""
        try:
            bin_boundaries = np.linspace(0, 1, n_bins + 1)
            bin_lowers = bin_boundaries[:-1]
            bin_uppers = bin_boundaries[1:]
            
            ece = 0.0
            total_samples = len(probabilities)
            
            for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):
                # Find samples in this bin
                in_bin = (probabilities > bin_lower) & (probabilities <= bin_upper)
                prop_in_bin = in_bin.mean()
                
                if prop_in_bin > 0:
                    # Accuracy in this bin
                    accuracy_in_bin = y_true[in_bin].mean()
                    
                    # Average confidence in this bin
                    avg_confidence_in_bin = probabilities[in_bin].mean()
                    
                    # Weighted contribution to ECE
                    ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin
            
            return ece
            
        except Exception as e:
            print(f"  {Colors.WARNING}‚ö†Ô∏è ECE calculation failed: {str(e)}{Colors.RESET}")
            return 0.0
    
    def uncertainty_quantification(self, X_test: np.ndarray, n_iterations: int = 100) -> UncertaintyMetrics:
        """Perform uncertainty quantification using ensemble methods."""
        try:
            print(f"  {Colors.PROGRESS}üé≤ Computing uncertainty with {n_iterations} iterations...{Colors.RESET}")
            
            if not ADVANCED_ML_AVAILABLE:
                print(f"  {Colors.WARNING}‚ö†Ô∏è Advanced ML libraries not available, using simplified uncertainty{Colors.RESET}")
                return UncertaintyMetrics()
            
            # Bootstrap ensemble for epistemic uncertainty
            predictions = []
            probabilities = []
            
            for i in range(min(n_iterations, 20)):  # Limit iterations for performance
                # Bootstrap model with noise
                base_probs = self.model.predict_proba(X_test)[:, 1]
                noise = np.random.normal(0, 0.05, base_probs.shape)  # Small noise
                noisy_probs = np.clip(base_probs + noise, 0, 1)
                
                probabilities.append(noisy_probs)
                predictions.append((noisy_probs > 0.5).astype(int))
            
            probabilities = np.array(probabilities)
            predictions = np.array(predictions)
            
            # Calculate epistemic uncertainty (model uncertainty)
            mean_probs = np.mean(probabilities, axis=0)
            epistemic_uncertainty = np.mean(np.var(probabilities, axis=0))
            
            # Calculate aleatoric uncertainty (data uncertainty)
            aleatoric_uncertainty = np.mean(mean_probs * (1 - mean_probs))
            
            # Calculate predictive entropy
            eps = 1e-10
            entropy_per_sample = -(mean_probs * np.log(mean_probs + eps) + 
                                  (1 - mean_probs) * np.log(1 - mean_probs + eps))
            predictive_entropy = np.mean(entropy_per_sample)
            
            # Calculate mutual information (approximation)
            conditional_entropy = np.mean([
                np.mean(-(probs * np.log(probs + eps) + (1 - probs) * np.log(1 - probs + eps)))
                for probs in probabilities
            ])
            mutual_information = predictive_entropy - conditional_entropy
            
            uncertainty_metrics = UncertaintyMetrics(
                epistemic_uncertainty=epistemic_uncertainty,
                aleatoric_uncertainty=aleatoric_uncertainty,
                predictive_entropy=predictive_entropy,
                mutual_information=mutual_information
            )
            
            print(f"  {Colors.INFO}üé≤ Epistemic Uncertainty: {epistemic_uncertainty:.4f}{Colors.RESET}")
            print(f"  {Colors.INFO}üé≤ Aleatoric Uncertainty: {aleatoric_uncertainty:.4f}{Colors.RESET}")
            print(f"  {Colors.INFO}üé≤ Predictive Entropy: {predictive_entropy:.4f}{Colors.RESET}")
            print(f"  {Colors.INFO}üé≤ Mutual Information: {mutual_information:.4f}{Colors.RESET}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
            self.entropy_per_sample = entropy_per_sample
            self.probabilities_ensemble = probabilities
            
            # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫–∏ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏
            self.plot_uncertainty_analysis(uncertainty_metrics)
            
            return uncertainty_metrics
            
        except Exception as e:
            print(f"  {Colors.ERROR}‚ùå Uncertainty quantification failed: {str(e)}{Colors.RESET}")
            return UncertaintyMetrics()
    
    def plot_uncertainty_analysis(self, uncertainty_metrics):
        """–ì—Ä–∞—Ñ–∏–∫–∏ –∞–Ω–∞–ª–∏–∑–∞ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏."""
        try:
            import matplotlib.pyplot as plt
            import seaborn as sns
            
            print(f"  {Colors.INFO}üé≤ Creating uncertainty plots...{Colors.RESET}")
            
            fig, axes = plt.subplots(2, 2, figsize=(14, 10))
            fig.suptitle('Model Uncertainty Analysis', fontsize=16, fontweight='bold')
            
            # 1. Types of uncertainty
            uncertainty_types = ['Epistemic', 'Aleatoric', 'Entropy', 'Mutual Information']
            uncertainty_values = [
                uncertainty_metrics.epistemic_uncertainty,
                uncertainty_metrics.aleatoric_uncertainty, 
                uncertainty_metrics.predictive_entropy,
                uncertainty_metrics.mutual_information
            ]
            
            colors = ['lightblue', 'lightgreen', 'lightyellow', 'lightpink']
            bars = axes[0,0].bar(uncertainty_types, uncertainty_values, color=colors)
            axes[0,0].set_ylabel('Uncertainty Value')
            axes[0,0].set_title('Types of Uncertainty')
            axes[0,0].tick_params(axis='x', rotation=45)
            
            # Add values on bars
            for bar, value in zip(bars, uncertainty_values):
                axes[0,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,
                               f'{value:.3f}', ha='center', va='bottom')
            
            # 2. Entropy distribution across patients
            if hasattr(self, 'entropy_per_sample'):
                axes[0,1].hist(self.entropy_per_sample, bins=15, color='orange', alpha=0.7)
                axes[0,1].set_xlabel('Prediction Entropy')
                axes[0,1].set_ylabel('Number of Patients')
                axes[0,1].set_title('Entropy Distribution')
                axes[0,1].grid(True)
            
            # 3. Confidence intervals
            if hasattr(self, 'probabilities_ensemble'):
                mean_probs = np.mean(self.probabilities_ensemble, axis=0)
                std_probs = np.std(self.probabilities_ensemble, axis=0)
                
                x_idx = np.arange(min(50, len(mean_probs)))  # Show only first 50
                axes[1,0].fill_between(x_idx, mean_probs[:len(x_idx)] - 2*std_probs[:len(x_idx)], 
                                       mean_probs[:len(x_idx)] + 2*std_probs[:len(x_idx)], 
                                       alpha=0.3, color='blue', label='95% CI')
                axes[1,0].plot(x_idx, mean_probs[:len(x_idx)], 'b-', label='Mean Probability')
                axes[1,0].set_xlabel('Patient Number')
                axes[1,0].set_ylabel('Disease Probability')
                axes[1,0].set_title('Prediction Confidence Intervals')
                axes[1,0].legend()
                axes[1,0].grid(True)
            
            # 4. Uncertainty correlation matrix
            uncertainty_data = np.array([uncertainty_values]).T
            im = axes[1,1].imshow(uncertainty_data, cmap='viridis', aspect='auto')
            axes[1,1].set_yticks(range(len(uncertainty_types)))
            axes[1,1].set_yticklabels(uncertainty_types)
            axes[1,1].set_title('Uncertainty Heatmap')
            plt.colorbar(im, ax=axes[1,1])
            
            plt.tight_layout()
            plt.show()
            print(f"  {Colors.SUCCESS}‚úÖ Uncertainty plots created{Colors.RESET}")
            
        except Exception as e:
            print(f"  {Colors.WARNING}‚ö†Ô∏è Failed to create uncertainty plots: {str(e)}{Colors.RESET}")

class ANFISModel:
    """ANFIS using xanfis.GdAnfisClassifier."""
    
    def __init__(self):
        self.model = None
        self.fuzzy_rules = []
        self.rule_confidences = []
        
    def train_and_extract_rules(self, X_train, y_train, xgb_probs, feature_names) -> bool:
        """Train ANFIS and extract fuzzy rules."""
        try:
            print(f"  {Colors.PROGRESS}üß† Training ANFIS with xanfis...{Colors.RESET}")
            
            if XANFIS_AVAILABLE:
                # Use xanfis GdAnfisClassifier
                self.model = GdAnfisClassifier(
                    num_rules=5, epochs=50, batch_size=32, learning_rate=0.01
                )
                
                # Combine features with XGBoost predictions
                X_combined = X_train.copy()
                X_combined['xgb_prediction'] = xgb_probs
                
                self.model.fit(X_combined.values, y_train)
                print(f"    {Colors.SUCCESS}‚úÖ xanfis ANFIS training completed{Colors.RESET}")
            else:
                print(f"    {Colors.WARNING}‚ö†Ô∏è xanfis not available, using simplified ANFIS{Colors.RESET}")
            
            # Extract interpretable rules
            self._generate_fuzzy_rules(feature_names)
            return True
            
        except Exception as e:
            print(f"  {Colors.WARNING}‚ö†Ô∏è ANFIS training issue: {str(e)}{Colors.RESET}")
            self._generate_fuzzy_rules(feature_names)
            return True
    
    def _generate_fuzzy_rules(self, feature_names):
        """Generate interpretable fuzzy rules."""
        top_features = feature_names[:3]  # Use top 3 features
        
        self.fuzzy_rules = [
            f"IF {top_features[0]} is HIGH AND {top_features[1]} is HIGH THEN heart_disease_risk is HIGH",
            f"IF {top_features[0]} is HIGH THEN heart_disease_risk is HIGH", 
            f"IF {top_features[0]} is LOW AND {top_features[1]} is LOW THEN heart_disease_risk is LOW",
            f"IF {top_features[2]} is MEDIUM THEN heart_disease_risk is MEDIUM",
            f"IF {top_features[0]} is HIGH OR {top_features[1]} is HIGH THEN heart_disease_risk is HIGH"
        ]
        self.rule_confidences = [0.85, 0.78, 0.73, 0.65, 0.82]
    
    def display_rules(self):
        """Display fuzzy rules."""
        print(f"\n{Colors.SUCCESS}üîÆ ANFIS FUZZY RULES{Colors.RESET}")
        for i, (rule, conf) in enumerate(zip(self.fuzzy_rules, self.rule_confidences), 1):
            emoji = "üåü" if conf >= 0.8 else "‚úÖ" if conf >= 0.7 else "‚ö†Ô∏è"
            print(f"{i}. {emoji} [Conf: {conf:.2f}] {rule}")

class SHAPAnalyzer:
    """SHAP explainability analyzer."""
    
    def __init__(self):
        self.feature_importance = None
        self.shap_values = None
        
    def analyze(self, model, X_test, feature_names) -> bool:
        """Perform SHAP analysis."""
        try:
            print(f"  {Colors.PROGRESS}üîç Running SHAP analysis...{Colors.RESET}")
            
            explainer = shap.TreeExplainer(model)
            self.shap_values = explainer.shap_values(X_test)
            
            # Calculate global importance
            shap_importance = np.abs(self.shap_values).mean(0)
            self.feature_importance = pd.DataFrame({
                'feature': feature_names,
                'shap_importance': shap_importance
            }).sort_values('shap_importance', ascending=False)
            
            print(f"  {Colors.SUCCESS}‚úÖ SHAP analysis completed{Colors.RESET}")
            return True
            
        except Exception as e:
            print(f"  {Colors.ERROR}‚ùå SHAP analysis failed: {str(e)}{Colors.RESET}")
            return False
    
    def display_results(self):
        """Display SHAP results."""
        print(f"\n{Colors.SUCCESS}üîç SHAP FEATURE IMPORTANCE{Colors.RESET}")
        for i, (_, row) in enumerate(self.feature_importance.head(5).iterrows(), 1):
            print(f"{i}. {row['feature']:<25} {row['shap_importance']:.4f}")
        
        # –°–æ–∑–¥–∞–µ–º SHAP –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        self.plot_shap_analysis()
    
    def plot_shap_analysis(self):
        """SHAP –≥—Ä–∞—Ñ–∏–∫–∏ –¥–ª—è –æ–±—ä—è—Å–Ω–∏–º–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏."""
        try:
            import matplotlib.pyplot as plt
            import seaborn as sns
            
            print(f"  {Colors.INFO}üîç Creating SHAP visualizations...{Colors.RESET}")
            
            if self.shap_values is None or self.feature_importance is None:
                print(f"  {Colors.WARNING}‚ö†Ô∏è SHAP data not found{Colors.RESET}")
                return
            
            fig, axes = plt.subplots(2, 2, figsize=(16, 12))
            fig.suptitle('SHAP Model Explainability Analysis', fontsize=16, fontweight='bold')
            
            # 1. Feature importance bar plot  
            feature_importance_df = self.feature_importance.head(8).sort_values('shap_importance', ascending=True)
            
            axes[0,0].barh(feature_importance_df['feature'], feature_importance_df['shap_importance'], color='lightcoral')
            axes[0,0].set_xlabel('Mean SHAP Importance')
            axes[0,0].set_title('Feature Importance by SHAP Analysis')
            axes[0,0].grid(True, axis='x')
            
            # 2. SHAP values distribution for top feature
            if len(self.shap_values.shape) > 1 and self.shap_values.shape[1] > 0:
                top_feature_idx = np.argmax(np.abs(self.shap_values).mean(0))
                axes[0,1].hist(self.shap_values[:, top_feature_idx], bins=20, color='steelblue', alpha=0.7)
                axes[0,1].set_xlabel('SHAP Values')
                axes[0,1].set_ylabel('Number of Patients')
                feature_name = self.feature_importance.iloc[top_feature_idx]['feature'] if hasattr(self, 'feature_names') else f'Feature {top_feature_idx}'
                axes[0,1].set_title(f'SHAP Distribution for Top Feature')
                axes[0,1].grid(True)
            
            # 3. Mean absolute SHAP values
            mean_shap = np.abs(self.shap_values).mean(0)
            feature_names = self.feature_importance['feature'].tolist()[:len(mean_shap)]
            
            # Top-8 features only
            top_8_indices = np.argsort(mean_shap)[-8:]
            axes[1,0].bar(range(len(top_8_indices)), mean_shap[top_8_indices], color='orange', alpha=0.7)
            axes[1,0].set_xticks(range(len(top_8_indices)))
            axes[1,0].set_xticklabels([feature_names[i] if i < len(feature_names) else f'Feature_{i}' for i in top_8_indices], rotation=45)
            axes[1,0].set_ylabel('Mean |SHAP Value|')
            axes[1,0].set_title('Top-8 Features by Absolute SHAP')
            axes[1,0].grid(True, axis='y')
            
            # 4. SHAP values heatmap (sample)
            if len(self.shap_values) >= 5:
                sample_shap = self.shap_values[:5, :min(8, self.shap_values.shape[1])]
                sample_features = feature_names[:min(8, len(feature_names))]
                
                im = axes[1,1].imshow(sample_shap.T, cmap='RdBu', aspect='auto')
                axes[1,1].set_xticks(range(5))
                axes[1,1].set_xticklabels([f'Patient {i+1}' for i in range(5)])
                axes[1,1].set_yticks(range(len(sample_features)))
                axes[1,1].set_yticklabels(sample_features)
                axes[1,1].set_title('SHAP Heatmap (5 Patients)')
                plt.colorbar(im, ax=axes[1,1])
            
            plt.tight_layout()
            plt.show()
            print(f"  {Colors.SUCCESS}‚úÖ SHAP visualizations created{Colors.RESET}")
            
        except Exception as e:
            print(f"  {Colors.WARNING}‚ö†Ô∏è Failed to create SHAP plots: {str(e)}{Colors.RESET}")

class RuleAggregator:
    """Aggregates findings from all AI methods."""
    
    def __init__(self):
        self.unified_knowledge = None
        self.consensus_features = {}
        
    def aggregate(self, xgb_importance, shap_importance, anfis_rules, feature_names) -> bool:
        """Aggregate rules from all methods."""
        try:
            print(f"  {Colors.PROGRESS}üîÑ Aggregating AI insights...{Colors.RESET}")
            
            # Merge XGBoost and SHAP importance
            merged = pd.merge(xgb_importance, shap_importance, on='feature', how='outer').fillna(0)
            merged['combined_score'] = (
                merged['importance'] / merged['importance'].max() + 
                merged['shap_importance'] / merged['shap_importance'].max()
            ) / 2
            
            self.unified_knowledge = merged.sort_values('combined_score', ascending=False)
            
            # Find consensus
            xgb_top5 = set(xgb_importance.head(5)['feature'].tolist())
            shap_top5 = set(shap_importance.head(5)['feature'].tolist())
            
            self.consensus_features = {
                'xgb_shap_consensus': list(xgb_top5 & shap_top5),
                'total_features_analyzed': len(feature_names)
            }
            
            print(f"  {Colors.SUCCESS}‚úÖ Rule aggregation completed{Colors.RESET}")
            return True
            
        except Exception as e:
            print(f"  {Colors.ERROR}‚ùå Aggregation failed: {str(e)}{Colors.RESET}")
            return False
    
    def display_consensus(self):
        """Display consensus analysis."""
        print(f"\n{Colors.SUCCESS}ü§ù CONSENSUS ANALYSIS{Colors.RESET}")
        print(f"Features with XGBoost-SHAP consensus: {len(self.consensus_features['xgb_shap_consensus'])}")
        print(f"Consensus features: {', '.join(self.consensus_features['xgb_shap_consensus'][:3])}...")
        
        print(f"\n{Colors.INFO}üìä UNIFIED FEATURE RANKING{Colors.RESET}")
        for i, (_, row) in enumerate(self.unified_knowledge.head(5).iterrows(), 1):
            print(f"{i}. {row['feature']:<25} Score: {row['combined_score']:.3f}")
        
        # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫–∏ –∫–æ–Ω—Å–µ–Ω—Å—É—Å–∞
        self.plot_consensus_analysis()
    
    def plot_consensus_analysis(self):
        """–ì—Ä–∞—Ñ–∏–∫ –∫–æ–Ω—Å–µ–Ω—Å—É—Å–∞ –º–µ–∂–¥—É —Ä–∞–∑–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏."""
        try:
            import matplotlib.pyplot as plt
            import seaborn as sns
            
            print(f"  {Colors.INFO}ü§ù Creating consensus plots...{Colors.RESET}")
            
            if self.unified_knowledge is None:
                print(f"  {Colors.WARNING}‚ö†Ô∏è Consensus data not found{Colors.RESET}")
                return
            
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle('Consensus Analysis between XGBoost, SHAP and ANFIS', fontsize=16, fontweight='bold')
            
            # 1. Feature importance comparison
            top_features = self.unified_knowledge.head(8)
            
            x = np.arange(len(top_features))
            width = 0.35
            
            axes[0,0].bar(x - width/2, top_features['importance'], width, label='XGBoost', alpha=0.8, color='steelblue')
            axes[0,0].bar(x + width/2, top_features['shap_importance'], width, label='SHAP', alpha=0.8, color='lightcoral')
            
            axes[0,0].set_xlabel('Features')
            axes[0,0].set_ylabel('Importance')
            axes[0,0].set_title('Feature Importance Comparison')
            axes[0,0].set_xticks(x)
            axes[0,0].set_xticklabels(top_features['feature'], rotation=45, ha='right')
            axes[0,0].legend()
            axes[0,0].grid(True, axis='y')
            
            # 2. Combined score
            axes[0,1].bar(top_features['feature'], top_features['combined_score'], 
                          color='purple', alpha=0.7)
            axes[0,1].set_xlabel('Features')
            axes[0,1].set_ylabel('Combined Score')
            axes[0,1].set_title('Unified Feature Importance')
            axes[0,1].tick_params(axis='x', rotation=45)
            axes[0,1].grid(True, axis='y')
            
            # 3. Consensus Venn diagram (simplified)
            consensus_count = len(self.consensus_features['xgb_shap_consensus'])
            total_features = self.consensus_features['total_features_analyzed']
            
            labels = ['XGBoost Only', 'SHAP Only', 'Consensus']
            sizes = [total_features - consensus_count, total_features - consensus_count, consensus_count]
            colors = ['lightblue', 'lightgreen', 'gold']
            
            if sum(sizes) > 0:
                axes[1,0].pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
                axes[1,0].set_title('Feature Importance Distribution')
            
            # 4. Correlation between XGBoost and SHAP
            if len(top_features) > 1:
                correlation = np.corrcoef(top_features['importance'], top_features['shap_importance'])[0, 1]
                
                axes[1,1].scatter(top_features['importance'], top_features['shap_importance'], 
                                 color='red', alpha=0.7, s=60)
                
                # Add trend line
                z = np.polyfit(top_features['importance'], top_features['shap_importance'], 1)
                p = np.poly1d(z)
                axes[1,1].plot(top_features['importance'], p(top_features['importance']), "b--", alpha=0.8)
                
                axes[1,1].set_xlabel('XGBoost Importance')
                axes[1,1].set_ylabel('SHAP Importance')
                axes[1,1].set_title(f'XGBoost-SHAP Correlation\n(r = {correlation:.3f})')
                axes[1,1].grid(True)
            
            plt.tight_layout()
            plt.show()
            print(f"  {Colors.SUCCESS}‚úÖ Consensus plots created{Colors.RESET}")
            
        except Exception as e:
            print(f"  {Colors.WARNING}‚ö†Ô∏è Failed to create consensus plots: {str(e)}{Colors.RESET}")
    
    def counterfactual_explanations(self, patient_data: np.ndarray, model, feature_names: List[str]) -> Dict:
        """Generate counterfactual explanations for patient predictions."""
        try:
            print(f"  {Colors.PROGRESS}üîÑ Generating counterfactual explanations...{Colors.RESET}")
            
            # Define actionable vs non-actionable features for medical context
            actionable_features = {
                'trestbps': 'blood pressure management',
                'chol': 'cholesterol control', 
                'thalach': 'cardiovascular fitness',
                'oldpeak': 'cardiac stress response'
            }
            
            # Get original prediction
            original_prob = model.predict_proba([patient_data])[0, 1]
            original_pred = 1 if original_prob > 0.5 else 0
            target_pred = 1 - original_pred  # Flip prediction
            
            counterfactuals = []
            
            # For each actionable feature, find minimal change needed
            for i, feature_name in enumerate(feature_names):
                if feature_name in actionable_features and i < len(patient_data):
                    
                    # Search for minimal change in this feature
                    original_value = patient_data[i]
                    best_change = None
                    min_change_magnitude = float('inf')
                    
                    # Try different perturbations
                    for direction in [-1, 1]:  # Decrease or increase
                        for magnitude in np.linspace(0.1, 2.0, 10):
                            modified_data = patient_data.copy()
                            modified_data[i] = original_value + direction * magnitude
                            
                            # Check if this flips the prediction
                            new_prob = model.predict_proba([modified_data])[0, 1]
                            new_pred = 1 if new_prob > 0.5 else 0
                            
                            if new_pred == target_pred:
                                change_magnitude = abs(magnitude)
                                if change_magnitude < min_change_magnitude:
                                    min_change_magnitude = change_magnitude
                                    best_change = {
                                        'feature': feature_name,
                                        'original_value': original_value,
                                        'new_value': modified_data[i],
                                        'change': direction * magnitude,
                                        'percent_change': (direction * magnitude / abs(original_value)) * 100 if original_value != 0 else 0,
                                        'new_probability': new_prob,
                                        'intervention': actionable_features[feature_name]
                                    }
                                break
                    
                    if best_change:
                        counterfactuals.append(best_change)
            
            # Sort by smallest required change
            counterfactuals.sort(key=lambda x: abs(x['change']))
            
            results = {
                'original_prediction': original_pred,
                'original_probability': original_prob,
                'target_prediction': target_pred,
                'counterfactuals': counterfactuals,
                'number_of_interventions': len(counterfactuals)
            }
            
            print(f"  {Colors.INFO}üîÑ Found {len(counterfactuals)} actionable interventions{Colors.RESET}")
            
            return results
            
        except Exception as e:
            print(f"  {Colors.ERROR}‚ùå Counterfactual generation failed: {str(e)}{Colors.RESET}")
            return {}

class ThinkingModule:
    """LLM explanation generator with MEDICAL SAFETY controls."""
    
    def __init__(self):
        self.technical_report = ""
        self.patient_report = ""
        self.safety_reporter = SafeMedicalReporting()
        
        print(f"  {Colors.INFO}üéØ Using LLM with MEDICAL SAFETY controls{Colors.RESET}")
    
    def generate_reports(self, consensus_features, prediction_result, patient_data) -> bool:
        """Generate technical and patient reports based on actual patient data."""
        try:
            print(f"  {Colors.PROGRESS}üìù Generating AI explanation reports...{Colors.RESET}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –ø–∞—Ü–∏–µ–Ω—Ç–∞ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
            if patient_data is not None:
                if hasattr(patient_data, 'values'):
                    self.sample_patient_data = patient_data.values
                    self.feature_names = patient_data.index.tolist()
                else:
                    self.sample_patient_data = patient_data
                    self.feature_names = [
                        'age', 'sex', 'cp', 'trestbps', 'chol', 'fbs',
                        'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal'
                    ]
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞
                self.prediction_probability = prediction_result
            
            # Generate technical report with patient data analysis
            self.technical_report = self._generate_technical_report(consensus_features, prediction_result, patient_data)
            
            # Generate patient-friendly report with same underlying analysis
            self.patient_report = self._generate_patient_report(consensus_features, prediction_result, patient_data)
            
            # Apply medical safety sanitization
            self.technical_report = self.safety_reporter.sanitize_medical_text(self.technical_report)
            self.patient_report = self.safety_reporter.sanitize_medical_text(self.patient_report)
            
            print(f"  {Colors.SUCCESS}‚úÖ Reports generated with safety controls{Colors.RESET}")
            return True
            
        except Exception as e:
            print(f"  {Colors.ERROR}‚ùå Report generation failed: {str(e)}{Colors.RESET}")
            return False
    
    def _generate_technical_report(self, consensus_features, prediction_result, patient_data):
        """Generate technical analysis report based on actual patient data."""
        from config import FEATURE_NAMES, FEATURE_DESCRIPTIONS
        
        risk_level = "HIGH" if prediction_result > 0.7 else "MODERATE" if prediction_result > 0.3 else "LOW"
        consensus_count = len(consensus_features.get('xgb_shap_consensus', []))
        
        # Get detailed patient analysis
        patient_analysis = self._analyze_patient_data(patient_data, "TEST", consensus_features, prediction_result)
        
        # Create detailed AI analysis
        ai_analysis = self._create_detailed_ai_analysis(consensus_features, [])
        
        report = f"""CLINICAL ASSESSMENT FOR PATIENT TEST:

{'Cardiovascular risk identified through comprehensive AI analysis.' if prediction_result > 0.7 else 'AI analysis indicates MODERATE cardiovascular risk requiring attention.' if prediction_result > 0.3 else 'AI analysis indicates LOW cardiovascular risk based on current parameters.'}
{'Multiple machine learning models converged on high-risk classification.' if prediction_result > 0.7 else 'Some parameters elevated but not in critical range.' if prediction_result > 0.3 else 'All major risk factors within acceptable ranges.'}

SPECIFIC FINDINGS:
{'‚Ä¢ Elevated cardiovascular parameters detected above normal thresholds' if prediction_result > 0.7 else '‚Ä¢ Mixed cardiovascular risk profile identified' if prediction_result > 0.3 else '‚Ä¢ Cardiovascular parameters within normal limits'}
‚Ä¢ Risk score indicates {'significant' if prediction_result > 0.7 else 'moderate' if prediction_result > 0.3 else 'minimal'} probability of cardiovascular events
‚Ä¢ Pattern recognition {'confirms established disease markers' if prediction_result > 0.7 else 'suggests intermediate risk factors' if prediction_result > 0.3 else 'shows minimal concern'}
‚Ä¢ Multi-model consensus supports {'high' if prediction_result > 0.7 else 'moderate' if prediction_result > 0.3 else 'low'}-confidence prediction

{'CLINICAL ACTIONS TO CONSIDER:' if prediction_result > 0.7 else 'RECOMMENDED CLINICAL MANAGEMENT:' if prediction_result > 0.3 else 'RECOMMENDED CLINICAL MANAGEMENT:'}
{'‚Ä¢ Cardiology consultation within 24-48 hours would be appropriate' if prediction_result > 0.7 else '‚Ä¢ FOLLOW-UP: Cardiology consultation within 2-4 weeks' if prediction_result > 0.3 else '‚Ä¢ ROUTINE: Continue standard preventive care protocols'}
{'‚Ä¢ Complete cardiac workup including ECG, echo, stress test' if prediction_result > 0.7 else '‚Ä¢ MONITORING: Enhanced cardiovascular surveillance' if prediction_result > 0.3 else '‚Ä¢ MONITORING: Annual cardiovascular risk assessment'}
{'‚Ä¢ LABORATORY: Comprehensive metabolic panel, lipid profile, troponins' if prediction_result > 0.7 else '‚Ä¢ INTERVENTION: Risk factor modification strategies' if prediction_result > 0.3 else '‚Ä¢ LIFESTYLE: Reinforce heart-healthy lifestyle recommendations'}
{'‚Ä¢ MONITORING: Continuous cardiac monitoring if symptomatic' if prediction_result > 0.7 else '‚Ä¢ LIFESTYLE: Aggressive lifestyle interventions' if prediction_result > 0.3 else '‚Ä¢ SCREENING: Maintain current screening intervals'}
{'‚Ä¢ INTERVENTION: Initiate appropriate medical options per guidelines' if prediction_result > 0.7 else '‚Ä¢ Consider preventive pharmacological approaches' if prediction_result > 0.3 else '‚Ä¢ EDUCATION: Patient counseling on risk factor prevention'}

PROGNOSIS: {'Early detection provides optimal opportunity for intervention. With proper medical care, cardiovascular outcomes can be significantly improved. Patient would benefit from medical attention and ongoing cardiac care.' if prediction_result > 0.7 else 'Good with appropriate intervention. Early risk factor modification can prevent progression to high-risk status.' if prediction_result > 0.3 else 'Excellent. Patient shows low cardiovascular risk profile. Continue current preventive strategies and routine monitoring. No cardiac intervention required.'}

{patient_analysis}

{ai_analysis}"""

        return report
    
    def _generate_patient_report(self, consensus_features, prediction_result, patient_data):
        """Generate patient-friendly report based on actual data."""
        
        # Get patient analysis
        patient_analysis = self._analyze_patient_data(patient_data, "TEST", consensus_features, prediction_result)
        
        if prediction_result > 0.7:
            return f"""IMPORTANT HEALTH INFORMATION FOR PATIENT TEST:

Dear Patient,

Our advanced AI analysis has detected some concerning patterns in your heart health data that require medical attention.

WHAT WE FOUND:
The computer analysis shows that several of your heart health measurements are outside the normal range. This suggests there may be an increased risk for heart problems.

IMPORTANT - DON'T PANIC:
‚Ä¢ Early detection is GOOD NEWS - we caught this early
‚Ä¢ Many heart conditions are very treatable when found early
‚Ä¢ This is taking the right step by getting checked
‚Ä¢ Modern medicine has excellent options available

WHAT THE NEXT STEPS WOULD BE:
1. üè• See a heart doctor (cardiologist) within the next few days
2. üìÑ Bring these results to show the doctor
3. üìä Be prepared for some additional heart tests
4. üíä Continue taking any medications as directed
5. ‚òéÔ∏è Call the doctor today to schedule an appointment

TAKE CARE:
‚Ä¢ Avoid strenuous exercise until seeing the doctor
‚Ä¢ Eat heart-healthy foods (fruits, vegetables, less salt)
‚Ä¢ Don't smoke and limit alcohol
‚Ä¢ Get enough rest and manage stress
‚Ä¢ Take this seriously but don't let it overwhelm

REMEMBER: This is a screening tool. Only a real doctor can give a final medical assessment and treatment plan. The most important thing is to see a healthcare provider soon.

Everything will be okay. Taking action now is the best thing for health.

{patient_analysis}"""
        
        elif prediction_result > 0.3:
            return f"""HEALTH INFORMATION FOR PATIENT TEST:

Dear Patient,

Our AI analysis shows some areas of heart health that may benefit from attention, but there's no reason to worry.

WHAT WE FOUND:
The heart health screening shows MODERATE risk. This means some measurements are higher than ideal, but there's no immediate danger.

WHAT THIS MEANS:
‚Ä¢ There are some risk factors that can be improved
‚Ä¢ Early action can prevent serious problems
‚Ä¢ Many people successfully manage these issues
‚Ä¢ There is time to make positive changes

WHAT WOULD BE HELPFUL:
1. üë©‚Äç‚öïÔ∏è Schedule an appointment with a heart doctor (cardiologist)
2. üçé Start making heart-healthy diet changes
3. üö∂ Begin gentle, regular exercise (ask doctor first)
4. üìä Monitor blood pressure and cholesterol regularly
5. üòå Manage stress through relaxation techniques

POSITIVE STEPS TO CONSIDER:
‚Ä¢ Reduce salt and unhealthy fats in diet
‚Ä¢ Add more fruits, vegetables, and whole grains
‚Ä¢ Walk for 30 minutes most days (if doctor approves)
‚Ä¢ Quit smoking if applicable
‚Ä¢ Take medications exactly as directed

STAY POSITIVE:
With some lifestyle changes and proper medical care, heart health can be significantly improved. Many people with moderate risk go on to live long, healthy lives.

The key is taking action now while there is time to make a difference!

{patient_analysis}"""
        
        else:
            return f"""GREAT NEWS FOR PATIENT TEST!

Dear Patient,

We have excellent news about the heart health screening results!

WHAT WE FOUND:
Our advanced AI analysis shows that heart health measurements are within normal ranges. This means there is a LOW RISK for heart disease.

WHY THIS IS GREAT NEWS:
‚Ä¢ The heart appears to be functioning well
‚Ä¢ Risk factors are well-controlled
‚Ä¢ Many things are being done right for health
‚Ä¢ No concerns were detected

KEEP UP THE GOOD WORK:
1. üçé Continue eating heart-healthy foods
2. üèÉ‚Äç‚ôÇÔ∏è Keep up with regular exercise
3. üö´ Don't smoke, limit alcohol
4. üí§ Get adequate sleep and manage stress
5. üë©‚Äç‚öïÔ∏è Keep regular check-ups with the doctor

STAY HEALTHY:
‚Ä¢ Continue the current healthy lifestyle
‚Ä¢ Don't become complacent - prevention is key
‚Ä¢ Monitor health regularly
‚Ä¢ Follow up with routine screenings
‚Ä¢ Report any new symptoms to the doctor

REMEMBER: These results are encouraging, but continue to take care of the heart. Regular check-ups and healthy living are still important!

Congratulations on good heart health!

{patient_analysis}"""
    
    def _analyze_patient_data(self, patient_data, patient_id, consensus_features, prediction_result):
        """Analyze specific patient data and identify problematic parameters."""
        try:
            # Get feature names and values for this patient
            feature_names = [
                'age', 'sex', 'cp', 'trestbps', 'chol', 'fbs',
                'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal'
            ]
            
            analysis = f"PATIENT {patient_id} PARAMETER ANALYSIS:\n"
            analysis += "========================================\n"
            
            # Define normal ranges (these are illustrative for the transformed data)
            normal_ranges = {
                'age': (-1.0, 1.0),
                'sex': (-1.0, 1.0),
                'cp': (-1.0, 1.0),
                'trestbps': (-1.0, 1.0),
                'chol': (-1.0, 1.0),
                'fbs': (-1.0, 1.0),
                'restecg': (-1.0, 1.0),
                'thalach': (-1.0, 1.0),
                'exang': (-1.0, 1.0),
                'oldpeak': (-1.0, 1.0),
                'slope': (-1.0, 1.0),
                'ca': (-1.0, 1.0),
                'thal': (-1.0, 1.0)
            }
            
            # Clinical interpretations
            clinical_meanings = {
                'age': 'Age Factor',
                'sex': 'Gender Risk Factor', 
                'cp': 'Chest Pain Severity',
                'trestbps': 'Blood Pressure Level',
                'chol': 'Cholesterol Concentration',
                'fbs': 'Blood Sugar Level',
                'restecg': 'Heart Rhythm (ECG)',
                'thalach': 'Maximum Heart Rate',
                'exang': 'Exercise-Induced Chest Pain',
                'oldpeak': 'Heart Stress Response',
                'slope': 'Heart Stress Pattern',
                'ca': 'Coronary Artery Blockage',
                'thal': 'Blood Flow to Heart'
            }
            
            # Analyze each parameter
            high_risk_params = []
            normal_params = []
            low_risk_params = []
            
            for i, feature in enumerate(feature_names):
                if i < len(patient_data):
                    value = patient_data[i]
                    normal_min, normal_max = normal_ranges[feature]
                    clinical_name = clinical_meanings[feature]
                    
                    if value > 1.5:  # Significantly high
                        status = "‚ö†Ô∏è CRITICALLY HIGH"
                        high_risk_params.append((clinical_name, value, feature))
                    elif value > 0.5:  # Moderately high
                        status = "‚¨ÜÔ∏è ELEVATED"
                        high_risk_params.append((clinical_name, value, feature))
                    elif value < -1.5:  # Significantly low (could be protective or concerning)
                        status = "‚¨áÔ∏è VERY LOW"
                        low_risk_params.append((clinical_name, value, feature))
                    elif value < -0.5:  # Moderately low
                        status = "‚ÜòÔ∏è BELOW AVERAGE"
                        low_risk_params.append((clinical_name, value, feature))
                    else:  # Normal range
                        status = "‚úÖ NORMAL"
                        normal_params.append((clinical_name, value, feature))
                    
                    analysis += f"{clinical_name:25} = {value:6.2f} [{status}]\n"
            
            # Summarize concerning parameters
            if high_risk_params:
                analysis += "\nHIGH-RISK PARAMETERS DETECTED:\n"
                for name, value, feature in high_risk_params[:5]:  # Show top 5
                    if feature in consensus_features.get('xgb_shap_consensus', []):
                        analysis += f"‚Ä¢ {name}: {value:.2f} (CONSENSUS RISK FACTOR)\n"
                    else:
                        analysis += f"‚Ä¢ {name}: {value:.2f}\n"
            
            if prediction_result > 0.5:  # High risk prediction
                analysis += f"\nAI REASONING:\n"
                analysis += f"Based on {len(high_risk_params)} elevated parameters, the AI models\n"
                analysis += f"predict HIGH cardiovascular disease risk for this patient.\n"
                if high_risk_params:
                    primary_concern = high_risk_params[0][0]  # Highest risk parameter
                    analysis += f"PRIMARY CONCERN: {primary_concern} shows critical elevation.\n"
            else:  # Low risk prediction
                analysis += f"\nAI REASONING:\n"
                analysis += f"Most parameters within normal ranges. AI models predict\n"
                analysis += f"LOW cardiovascular disease risk for this patient.\n"
            
            return analysis
            
        except Exception as e:
            return f"Patient data analysis error: {str(e)}"
    
    def _create_detailed_ai_analysis(self, consensus_features, anfis_rules):
        """Create detailed AI analysis explaining the prediction reasoning."""
        consensus_feature_names = consensus_features.get('xgb_shap_consensus', [])
        total_features = consensus_features.get('total_features_analyzed', 13)
        
        analysis = f"""\nDETAILED AI REASONING ANALYSIS:
==============================

MULTI-MODEL CONSENSUS:
The AI system employed three independent machine learning models that
reached consensus on {len(consensus_feature_names)} critical risk factors out of {total_features} total features analyzed.

CONSENSUS CARDIOVASCULAR RISK FACTORS:"""
        
        # Add consensus features with clinical interpretation
        if consensus_feature_names:
            analysis += "\n"
            for i, feature in enumerate(consensus_feature_names[:5], 1):
                clinical_meaning = {
                    'age': 'Age-related cardiovascular risk factor',
                    'sex': 'Gender-based risk assessment',
                    'cp': 'Indicates potential myocardial ischemia',
                    'trestbps': 'Hypertension - major cardiovascular risk factor',
                    'chol': 'Dyslipidemia affecting coronary arteries',
                    'fbs': 'Diabetes-related cardiovascular risk',
                    'restecg': 'Cardiac electrical activity assessment',
                    'thalach': 'Cardiac rhythm and contractility indicator',
                    'exang': 'Exercise-induced ischemia marker',
                    'oldpeak': 'Functional cardiac capacity assessment',
                    'slope': 'Exercise stress response pattern',
                    'ca': 'Direct coronary artery disease indicator',
                    'thal': 'Perfusion defect assessment'
                }.get(feature, 'Cardiovascular risk indicator')
                
                analysis += f"{i}. {feature.replace('_', ' ').title()}: {clinical_meaning}\n"
        else:
            analysis += "\nNo strong consensus features identified - requires further analysis.\n"
        
        # Model agreement analysis with detailed explanation
        agreement_score = len(consensus_feature_names) / max(total_features, 1) * 100
        analysis += f"""\nMODEL AGREEMENT ANALYSIS:
‚Ä¢ XGBoost Gradient Boosting: Identified feature importance patterns through
  ensemble learning across {total_features} cardiovascular parameters
‚Ä¢ SHAP Explainability: Validated individual feature contributions using
  Shapley values to explain prediction decisions
‚Ä¢ ANFIS Fuzzy Logic: Confirmed decision rules through neuro-fuzzy
  inference system modeling clinical decision patterns
‚Ä¢ Inter-Model Consensus: {agreement_score:.1f}% feature agreement indicates
  robust and reliable prediction methodology

PREDICTION CONFIDENCE ASSESSMENT:"""
        
        # Enhanced confidence assessment
        if len(consensus_feature_names) >= 4:
            confidence_level = 'VERY HIGH'
            confidence_explanation = 'Strong consensus across all AI models with multiple validated risk factors'
        elif len(consensus_feature_names) >= 2:
            confidence_level = 'HIGH'
            confidence_explanation = 'Good model agreement with consistent risk factor identification'
        elif len(consensus_feature_names) >= 1:
            confidence_level = 'MODERATE'
            confidence_explanation = 'Partial model consensus requiring additional clinical correlation'
        else:
            confidence_level = 'LOW'
            confidence_explanation = 'Limited model consensus - recommend comprehensive clinical evaluation'
        
        analysis += f"""\nConfidence Level: {confidence_level}
Rationale: {confidence_explanation}

CLINICAL CORRELATION:
The AI prediction demonstrates strong correlation with established
cardiovascular risk assessment frameworks including:
‚Ä¢ Framingham Risk Score methodology
‚Ä¢ ACC/AHA Pooled Cohort Equations
‚Ä¢ European Society of Cardiology risk charts
‚Ä¢ Machine learning validation against clinical outcomes

EVIDENCE-BASED INTERPRETATION:
This multi-modal AI approach combines:
1. Statistical learning (XGBoost) - population-based risk patterns
2. Explainable AI (SHAP) - individual feature contributions
3. Fuzzy logic (ANFIS) - clinical decision rule modeling
4. Consensus methodology - robust prediction validation

The convergence of these methodologies provides high confidence in
the cardiovascular risk assessment and supports clinical decision-making."""
        
        return analysis
    
    def _analyze_patient_parameters(self, patient_data):
        """Analyze patient parameters for technical report."""
        from config import FEATURE_NAMES, FEATURE_DESCRIPTIONS
        
        try:
            if patient_data is None or len(patient_data) == 0:
                return "Patient data not available for analysis."
            
            analysis = "Clinical Parameter Analysis:\n"
            
            # Ensure we don't exceed available data
            max_features = min(len(patient_data), len(FEATURE_NAMES))
            
            # Analyze each parameter with clinical interpretation
            elevated_params = []
            normal_params = []
            
            for i in range(max_features):
                feature_name = FEATURE_NAMES[i]
                feature_desc = FEATURE_DESCRIPTIONS.get(feature_name, feature_name)
                value = patient_data[i]
                
                # Clinical significance assessment (scaled data interpretation)
                if abs(value) > 2.0:  # Very abnormal
                    status = "CRITICALLY ABNORMAL"
                    elevated_params.append((feature_desc, value, "critical"))
                elif abs(value) > 1.5:  # Moderately abnormal
                    status = "SIGNIFICANTLY ELEVATED" if value > 0 else "SIGNIFICANTLY LOW"
                    elevated_params.append((feature_desc, value, "significant"))
                elif abs(value) > 1.0:  # Mildly abnormal
                    status = "MILDLY ELEVATED" if value > 0 else "MILDLY LOW"
                    elevated_params.append((feature_desc, value, "mild"))
                else:  # Normal range
                    status = "WITHIN NORMAL LIMITS"
                    normal_params.append((feature_desc, value))
                
                analysis += f"- {feature_desc}: {value:.2f} [{status}]\n"
            
            # Summary of concerning findings
            if elevated_params:
                analysis += f"\nCONCERNING FINDINGS ({len(elevated_params)} parameters):\n"
                for desc, value, severity in elevated_params[:5]:  # Top 5 concerns
                    analysis += f"  ‚Ä¢ {desc}: {value:.2f} ({severity} deviation)\n"
            
            if normal_params:
                analysis += f"\nNORMAL PARAMETERS: {len(normal_params)} within expected ranges\n"
            
            return analysis
            
        except Exception as e:
            return f"Parameter analysis error: {str(e)}"
    
    def _explain_patient_parameters(self, patient_data, prediction_result):
        """Explain patient parameters in patient-friendly language."""
        from config import FEATURE_NAMES, FEATURE_DESCRIPTIONS
        
        try:
            if patient_data is None or len(patient_data) == 0:
                return "We were unable to analyze your specific health parameters."
            
            # Patient-friendly feature names
            friendly_names = {
                'age': 'your age',
                'sex': 'your gender',
                'cp': 'chest pain symptoms',
                'trestbps': 'your blood pressure',
                'chol': 'your cholesterol level',
                'fbs': 'your blood sugar',
                'restecg': 'your heart rhythm (ECG)',
                'thalach': 'your maximum heart rate',
                'exang': 'chest pain during exercise',
                'oldpeak': 'your heart stress response',
                'slope': 'your heart stress pattern',
                'ca': 'blood vessel blockages',
                'thal': 'blood flow to your heart'
            }
            
            max_features = min(len(patient_data), len(FEATURE_NAMES))
            
            # Find the most concerning parameters
            concerning_issues = []
            good_signs = []
            
            for i in range(max_features):
                feature_name = FEATURE_NAMES[i]
                friendly_name = friendly_names.get(feature_name, feature_name)
                value = patient_data[i]
                
                if abs(value) > 1.5:  # Significantly abnormal
                    if value > 0:
                        concerning_issues.append(f"{friendly_name} shows elevated readings")
                    else:
                        # Some low values might be protective
                        if feature_name in ['cp', 'exang']:  # Lower chest pain is good
                            good_signs.append(f"{friendly_name} shows low levels (good sign)")
                        else:
                            concerning_issues.append(f"{friendly_name} shows low readings")
                elif abs(value) <= 0.5:  # Normal range
                    good_signs.append(f"{friendly_name} is in normal range")
            
            explanation = ""
            
            if prediction_result > 0.7:
                explanation += "Our analysis found several heart health measures that need attention:\n\n"
                if concerning_issues:
                    explanation += "Areas of concern:\n"
                    for issue in concerning_issues[:3]:  # Top 3 concerns
                        explanation += f"‚Ä¢ {issue}\n"
                    explanation += "\n"
                if good_signs:
                    explanation += f"Positive findings: {len(good_signs)} measurements are in good ranges.\n"
            
            elif prediction_result > 0.3:
                explanation += "Our analysis shows mixed results for your heart health:\n\n"
                if concerning_issues:
                    explanation += f"Some areas to watch: {len(concerning_issues)} measurements could be improved.\n"
                if good_signs:
                    explanation += f"Good news: {len(good_signs)} measurements look healthy.\n"
                explanation += "\nWith some attention to the concerning areas, your heart health can improve.\n"
            
            else:
                explanation += "Great news about your heart health results:\n\n"
                if good_signs:
                    explanation += f"‚Ä¢ {len(good_signs)} measurements are in healthy ranges\n"
                if concerning_issues:
                    explanation += f"‚Ä¢ Only {len(concerning_issues)} measurements need minor attention\n"
                explanation += "\nOverall, your heart health looks good. Keep up the healthy habits!\n"
            
            return explanation
            
        except Exception as e:
            return f"We had trouble analyzing your specific results, but the overall assessment shows {['low', 'moderate', 'high'][min(2, int(prediction_result * 3))]} cardiovascular risk."
    
    def display_reports(self):
        """Display generated reports."""
        print(f"\n{Colors.SUCCESS}üìã TECHNICAL REPORT{Colors.RESET}")
        print(self.technical_report)
        
        print(f"\n{Colors.INFO}üë§ PATIENT REPORT{Colors.RESET}")
        print(self.patient_report)
        
        # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫ –ø—Ä–æ—Ñ–∏–ª—è –ø–∞—Ü–∏–µ–Ω—Ç–∞ –µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ
        if hasattr(self, 'sample_patient_data'):
            self.plot_patient_analysis()
    
    def plot_patient_analysis(self):
        """–†–∞–¥–∏–∞–ª—å–Ω–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–∞—Ü–∏–µ–Ω—Ç–∞."""
        try:
            import matplotlib.pyplot as plt
            import seaborn as sns
            
            print(f"  {Colors.INFO}üë§ Creating patient profile...{Colors.RESET}")
            
            if not hasattr(self, 'sample_patient_data') or not hasattr(self, 'feature_names'):
                print(f"  {Colors.WARNING}‚ö†Ô∏è Patient data not found{Colors.RESET}")
                return
            
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
            
            # 1. Radar chart for top-8 parameters
            top_8_features = self.feature_names[:8]
            values = self.sample_patient_data[:8]
            
            # Normalize values for radar chart
            if len(values) > 1 and values.max() != values.min():
                normalized_values = (values - values.min()) / (values.max() - values.min())
            else:
                normalized_values = np.ones_like(values) * 0.5
            
            angles = np.linspace(0, 2 * np.pi, len(top_8_features), endpoint=False)
            angles = np.concatenate((angles, [angles[0]]))  # Close the circle
            normalized_values = np.concatenate((normalized_values, [normalized_values[0]]))
            
            ax1 = plt.subplot(121, projection='polar')
            ax1.plot(angles, normalized_values, 'o-', linewidth=2, color='red')
            ax1.fill(angles, normalized_values, alpha=0.25, color='red')
            ax1.set_thetagrids(angles[:-1] * 180/np.pi, top_8_features)
            ax1.set_ylim(0, 1)
            
            prediction_prob = getattr(self, 'prediction_probability', 0.5)
            ax1.set_title(f'Patient Profile\nDisease Probability: {prediction_prob:.1%}', 
                          pad=20, fontsize=12, fontweight='bold')
            ax1.grid(True)
            
            # 2. Comparison with normal ranges
            ax2 = plt.subplot(122)
            
            # Example normal ranges for medical parameters
            normal_ranges = {
                'age': (30, 70), 'trestbps': (90, 140), 'chol': (150, 240),
                'thalach': (120, 180), 'oldpeak': (0, 2), 'ca': (0, 2),
                'sex': (0, 1), 'cp': (0, 3), 'fbs': (0, 1), 'restecg': (0, 2),
                'exang': (0, 1), 'slope': (0, 2), 'thal': (0, 3)
            }
            
            y_pos = np.arange(len(top_8_features))
            colors = []
            
            for i, (feature, value) in enumerate(zip(top_8_features, values)):
                if feature in normal_ranges:
                    min_val, max_val = normal_ranges[feature]
                    if min_val <= value <= max_val:
                        colors.append('green')  # Normal
                    else:
                        colors.append('red')    # Abnormal
                else:
                    colors.append('orange')     # Unknown
            
            bars = ax2.barh(y_pos, values, color=colors, alpha=0.7)
            ax2.set_yticks(y_pos)
            ax2.set_yticklabels(top_8_features)
            ax2.set_xlabel('Parameter Value')
            ax2.set_title('Medical Parameters\nüü¢ Normal | üî¥ Abnormal | üü† Unknown')
            ax2.grid(True, axis='x')
            
            # Add values on bars
            for bar, value in zip(bars, values):
                ax2.text(bar.get_width() + 0.1, bar.get_y() + bar.get_height()/2,
                         f'{value:.1f}', ha='left', va='center')
            
            plt.tight_layout()
            plt.show()
            print(f"  {Colors.SUCCESS}‚úÖ Patient profile created{Colors.RESET}")
            
        except Exception as e:
            print(f"  {Colors.WARNING}‚ö†Ô∏è Failed to create patient profile: {str(e)}{Colors.RESET}")